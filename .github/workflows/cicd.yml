# .github/workflows/cicd.yml

name: CI/CD Pipeline

on:
  workflow_dispatch # Only run manually from the Actions tab
  # push:
  #   branches: [main, develop]
  # pull_request:
  #   branches: [main, develop]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
  
env:
  REGISTRY: ${{ vars.REGISTRY_URL }}
  IMAGE_NAME: ${{ vars.IMAGE_NAME }}
  K8S_NAMESPACE_PROD: ${{ vars.K8S_NAMESPACE_PROD }}
  K8S_NAMESPACE_STAGING: ${{ vars.K8S_NAMESPACE_STAGING }}

jobs:
  # Security Scanning and Code Quality
  security-scan:
    name: Code Quality & Security Scan
    runs-on: jen2
    # Ensure all steps execute inside the app folder
    # defaults:
    #   run:
    #     working-directory: ./app
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'app/package-lock.json'

      - name: Install deps and Run tests
        working-directory: ./app
        run: |
          npm ci
          npm test -- --coverage
          npm run test:integration || true
          npm run lint || true

      - name: Upload coverage reports
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage/coverage-final.json
          flags: unittests
          name: codecov-umbrella
          token: ${{ secrets.CODECOV_TOKEN }}
        continue-on-error: true

      - name: Test Report
        uses: dorny/test-reporter@v1
        # if: success() || failure()
        with:
          name: Test Results
          path: 'test-results/*.xml'
          reporter: jest-junit
        continue-on-error: true
  
      - name: Run SonarQube Scan
        uses: sonarsource/sonarqube-scan-action@master
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}
        continue-on-error: true
        
      - name: SAST - Semgrep
        uses: docker://semgrep/semgrep:latest
        with:
          args: >
            semgrep scan
            --config p/security-audit
            --config p/secrets
            --json
            --output semgrep-report.json
        continue-on-error: true

      - name: Upload SAST Report
        uses: actions/upload-artifact@v4
        with:
          name: sast-report
          path: semgrep-report.json

      - name: Run Snyk Security Scan
        uses: snyk/actions/node@master
        continue-on-error: true
        with:
          args: app k8s terraform --json-file-output=snyk-report.json
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}

      - name: Run GitLeaks (Secret Scanning)
        uses: gitleaks/gitleaks-action@v2
        continue-on-error: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: OWASP Dependency Check
        uses: dependency-check/Dependency-Check_Action@main
        with:
          project: ${{ vars.IMAGE_NAME }}
          path: './app'
          format: 'HTML'
        env:
          JAVA_HOME: /opt/jdk

      - name: Upload OWASP Scan Report
        uses: actions/upload-artifact@v4
        with:
          name: dependency-check-report
          path: reports/dependency-check-report.html # Path must match the 'out' setting above

      - name: Fix Permissions
        if: always() # This is crucial: it runs even if the step above fails
        working-directory: .
        run: |
          sudo chown -R $USER:$USER ./reports || true
 
  # Build and Push Docker Image
  build-and-push:
    name: Build & Push Docker Image
    runs-on: jen2
    needs: security-scan
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    outputs:
      image-version: ${{ steps.meta.outputs.version }}
      image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.meta.outputs.version }}
      image-digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          buildkitd-config-inline: |
            [registry."${{ env.REGISTRY }}"]
              http = true

      - name: Log in to Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.REGISTRY_USERNAME }}
          password: ${{ secrets.REGISTRY_PASSWORD }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./app
          file: ./app/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache
          cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache,mode=max
          build-args: |
            BUILD_DATE=${{ steps.meta.outputs.created }}
            VCS_REF=${{ github.sha }}
            VERSION=${{ steps.meta.outputs.version }}

  # Job 4: Container Security Scan
  container-scan:
    name: Scan Container for Vulnerabilities
    runs-on: jen2
    needs: build-and-push
    steps:
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ needs.build-and-push.outputs.image }}
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          vuln-type: 'os'
        env:
          TRIVY_INSECURE: "true"
        continue-on-error: true
        

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'
        continue-on-error: true

      - name: Run Trivy in table format
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ needs.build-and-push.outputs.image }}
          format: 'table'
          exit-code: '0'
          severity: 'CRITICAL,HIGH'
          vuln-type: 'os'

      # - name: Sign container image with Cosign
      #   env:
      #     COSIGN_PRIVATE_KEY: ${{ secrets.COSIGN_PRIVATE_KEY }}
      #     COSIGN_PASSWORD: ${{ secrets.COSIGN_PASSWORD }}
      #   run: |
      #     echo "${COSIGN_PRIVATE_KEY}" > cosign.key
      #     cosign sign --key cosign.key \
      #       ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}@${{ steps.build.outputs.digest }}
      #     rm -f cosign.key

  # Deploy to K3s
  deploy:
    name: Deploy to K3s
    runs-on: buildserver1
    needs: [build-and-push, container-scan]
    if: github.ref == 'refs/heads/main' && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # - name: Set up kubectl
      #   run: |
      #     mkdir -p $HOME/.kube
      #     echo "${{ secrets.KUBECONFIG }}" | base64 -d > $HOME/.kube/config
      #     chmod 600 $HOME/.kube/config
      #     echo "KUBECONFIG=$(realpath $HOME/.kube/config)" >> $GITHUB_ENV
          
      - name: Verify kubectl connection
        run: |
          kubectl version --client
          kubectl cluster-info

      - name: Deploy to Kubernetes
        run: |
          # Set variables from environment
          IMAGE_VERSION="${{ needs.build-and-push.outputs.image-version }}"
          APP_NAME="${{ env.IMAGE_NAME }}"
          NAMESPACE="${{ env.K8S_NAMESPACE_PROD }}"
          REGISTRY="${{ env.REGISTRY }}"
          
          echo "Deploying configuration:"
          echo "  App Name: $APP_NAME"
          echo "  Namespace: $NAMESPACE"
          echo "  Registry: $REGISTRY"
          echo "  Image Tag: $IMAGE_VERSION"
          
          # Replace placeholders in all manifest files
          for file in k8s/*.yml; do
            echo "Processing $file..."
            sed -i "s|__APP_NAME__|${APP_NAME}|g" "$file"
            sed -i "s|__NAMESPACE__|${NAMESPACE}|g" "$file"
            sed -i "s|__REGISTRY__|${REGISTRY}|g" "$file"
            sed -i "s|__IMAGE_NAME__|${APP_NAME}|g" "$file"
            sed -i "s|__IMAGE_TAG__|${IMAGE_VERSION}|g" "$file"
          done
          
          # Verify the replacement worked
          echo "Checking deployment manifest:"
          grep "image:" k8s/deployment.yml
          grep "namespace:" k8s/deployment.yml | head -1

          # Apply Kubernetes manifests
          kubectl apply -f k8s/namespace.yml
          kubectl apply -f k8s/configmap.yml
          kubectl apply -f k8s/secret.yml
          kubectl apply -f k8s/service.yml
          kubectl apply -f k8s/deployment.yml
          kubectl apply -f k8s/hpa.yml
          
          # Wait for rollout
          kubectl rollout status deployment/${{ env.IMAGE_NAME }} -n ${{ env.K8S_NAMESPACE_PROD }} --timeout=5m

      - name: Verify deployment
        run: |
          kubectl get pods -n ${{ env.K8S_NAMESPACE_PROD }}
          kubectl get svc -n ${{ env.K8S_NAMESPACE_PROD }}
          kubectl get ingress -n ${{ env.K8S_NAMESPACE_PROD }}

      - name: Run smoke tests
        run: |
          SERVICE_NAME="${{ env.IMAGE_NAME }}-service"
          NAMESPACE="${{ env.K8S_NAMESPACE_PROD }}"

          echo "Fetching service details..."
          kubectl get service $SERVICE_NAME -n $NAMESPACE

          NODE_PORT=$(kubectl get service $SERVICE_NAME -n $NAMESPACE -o jsonpath='{.spec.ports[0].nodePort}')
          NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="ExternalIP")].address}')  # Use ExternalIP; fallback to InternalIP if no public IP

          if [ -z "$NODE_IP" ]; then
            NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
          fi

          echo "Testing on Node IP: $NODE_IP and Port: $NODE_PORT"

          sleep 30

          # Test with verbose output and proper error handling
          echo "Testing /health endpoint..."
          if curl -v -f --max-time 10 "http://$NODE_IP:$NODE_PORT/health"; then
            echo "✅ Health check passed"
          else
            echo "❌ Health check failed"
            echo "Checking pod status..."
            kubectl describe pod -l app=${{ env.IMAGE_NAME }} -n $NAMESPACE
            exit 1
          fi

          echo "Testing /ready endpoint..."
          curl -f "http://$NODE_IP:$NODE_PORT/ready" || echo "Ready check failed"

          echo "Testing /api/users endpoint..."
          curl -f "http://$NODE_IP:$NODE_PORT/api/users" || echo "API check failed"



  # Rollback on Failure
  rollback:
    name: Rollback on Failure
    runs-on: buildserver1
    needs: deploy
    if: failure()
    steps:
      - name: Set up kubectl
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > $HOME/.kube/config
          chmod 600 $HOME/.kube/config
          
      - name: Rollback deployment
        run: |
          kubectl rollout undo deployment/${{ env.IMAGE_NAME }} -n ${{ env.K8S_NAMESPACE_PROD }}
          kubectl rollout status deployment/${{ env.IMAGE_NAME }} -n ${{ env.K8S_NAMESPACE_PROD }}
